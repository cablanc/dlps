{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyro-ppca.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOlduUb5MYjEffvJCgL7seM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cablanc/dlps/blob/master/pyro_ppca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8SCFX3fnYWZ"
      },
      "source": [
        "[TF intro to probabilistic programming](https://blog.tensorflow.org/2018/12/an-introduction-to-probabilistic.html)\n",
        "[Probabilistic Programming and Bayesian Methods for Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/tree/master/Chapter2_MorePyMC)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0QJDj1y0LlE",
        "outputId": "03334a9d-59d0-4fb1-bcc6-cab1351815de"
      },
      "source": [
        "!pip3 install pyro-ppl "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyro-ppl in /usr/local/lib/python3.7/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (0.1.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (1.8.0+cu101)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (4.41.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.0->pyro-ppl) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11tslkNpl3J7",
        "outputId": "30aaeb4d-91a5-40fd-b1de-83909c1d1582"
      },
      "source": [
        "!pip install visdom"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: visdom in /usr/local/lib/python3.7/dist-packages (0.1.8.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visdom) (2.23.0)\n",
            "Requirement already satisfied: torchfile in /usr/local/lib/python3.7/dist-packages (from visdom) (0.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from visdom) (1.15.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from visdom) (5.1.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.7/dist-packages (from visdom) (0.58.0)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.7/dist-packages (from visdom) (1.32)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from visdom) (7.0.0)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom) (22.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from visdom) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.7/dist-packages (from visdom) (1.19.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (2020.12.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.7/dist-packages (from jsonpatch->visdom) (2.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aD9TaRBDL1U",
        "outputId": "e5f33f04-0372-4237-d294-1576a0f14fcf"
      },
      "source": [
        "mkdir vae_results"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‚Äòvae_results‚Äô: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZj93Z-mywQo"
      },
      "source": [
        "For a model with ùëÅ observations, running the model and guide and constructing the ELBO involves evaluating log pdf‚Äôs whose complexity scales badly with ùëÅ. This is a problem if we want to scale to large datasets. Luckily, the ELBO objective naturally supports subsampling provided that our model/guide have some conditional independence structure that we can take advantage of. For example, in the case that the observations are conditionally independent given the latents, the log likelihood term in the ELBO can be approximated (see [Autoencoding Variational Bayes](https://arxiv.org/abs/1312.6114))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDrr7iaRoAwM"
      },
      "source": [
        "In Pyro, the model corresponds to p(x,z;Œ∏) and the guide corresponds to q(z|x;œï)\n",
        "\n",
        "Pyro calculates p(x, z) = p(x|z)p(z), where z is a sample from q(z)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lsCduY2II_4"
      },
      "source": [
        "import argparse\n",
        "import errno\n",
        "import os\n",
        "from functools import reduce\n",
        "import visdom\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from pyro.contrib.examples.util import MNIST, get_data_directory\n",
        "\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "from pyro.infer import SVI, JitTrace_ELBO, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "\n",
        "assert pyro.__version__.startswith('1.6.0')\n",
        "# parse command line arguments\n",
        "import sys"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83xd7_EM8iDd"
      },
      "source": [
        "# Copyright (c) 2017-2019 Uber Technologies, Inc.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "def plot_conditional_samples_ssvae(ssvae, visdom_session):\n",
        "    \"\"\"\n",
        "    This is a method to do conditional sampling in visdom\n",
        "    \"\"\"\n",
        "    vis = visdom_session\n",
        "    ys = {}\n",
        "    for i in range(10):\n",
        "        ys[i] = torch.zeros(1, 10)\n",
        "        ys[i][0, i] = 1\n",
        "    xs = torch.zeros(1, 784)\n",
        "\n",
        "    for i in range(10):\n",
        "        images = []\n",
        "        for rr in range(100):\n",
        "            # get the loc from the model\n",
        "            sample_loc_i = ssvae.model(xs, ys[i])\n",
        "            img = sample_loc_i[0].view(1, 28, 28).cpu().data.numpy()\n",
        "            images.append(img)\n",
        "        vis.images(images, 10, 2)\n",
        "\n",
        "\n",
        "def plot_llk(train_elbo, test_elbo):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import scipy as sp\n",
        "    import seaborn as sns\n",
        "    plt.figure(figsize=(30, 10))\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    data = np.concatenate([np.arange(len(test_elbo))[:, sp.newaxis], -test_elbo[:, sp.newaxis]], axis=1)\n",
        "    df = pd.DataFrame(data=data, columns=['Training Epoch', 'Test ELBO'])\n",
        "    g = sns.FacetGrid(df, size=10, aspect=1.5)\n",
        "    g.map(plt.scatter, \"Training Epoch\", \"Test ELBO\")\n",
        "    g.map(plt.plot, \"Training Epoch\", \"Test ELBO\")\n",
        "    plt.savefig('./vae_results/test_elbo_vae.png')\n",
        "    plt.close('all')\n",
        "\n",
        "\n",
        "def plot_vae_samples(vae, visdom_session):\n",
        "    vis = visdom_session\n",
        "    x = torch.zeros([1, 784])\n",
        "    for i in range(10):\n",
        "        images = []\n",
        "        for rr in range(100):\n",
        "            # get loc from the model\n",
        "            sample_loc_i = vae.model(x)\n",
        "            img = sample_loc_i[0].view(1, 28, 28).cpu().data.numpy()\n",
        "            images.append(img)\n",
        "        vis.images(images, 10, 2)\n",
        "\n",
        "\n",
        "def mnist_test_tsne(vae=None, test_loader=None):\n",
        "    \"\"\"\n",
        "    This is used to generate a t-sne embedding of the vae\n",
        "    \"\"\"\n",
        "    name = 'VAE'\n",
        "    data = test_loader.dataset.test_data.float()\n",
        "    mnist_labels = test_loader.dataset.test_labels\n",
        "    z_loc, z_scale = vae.encoder(data)\n",
        "    plot_tsne(z_loc, mnist_labels, name)\n",
        "\n",
        "\n",
        "def mnist_test_tsne_ssvae(name=None, ssvae=None, test_loader=None):\n",
        "    \"\"\"\n",
        "    This is used to generate a t-sne embedding of the ss-vae\n",
        "    \"\"\"\n",
        "    if name is None:\n",
        "        name = 'SS-VAE'\n",
        "    data = test_loader.dataset.test_data.float()\n",
        "    mnist_labels = test_loader.dataset.test_labels\n",
        "    z_loc, z_scale = ssvae.encoder_z([data, mnist_labels])\n",
        "    plot_tsne(z_loc, mnist_labels, name)\n",
        "\n",
        "\n",
        "def plot_tsne(z_loc, classes, name):\n",
        "    import matplotlib\n",
        "    matplotlib.use('Agg')\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    from sklearn.manifold import TSNE\n",
        "    model_tsne = TSNE(n_components=2, random_state=0)\n",
        "    z_states = z_loc.detach().cpu().numpy()\n",
        "    z_embed = model_tsne.fit_transform(z_states)\n",
        "    classes = classes.detach().cpu().numpy()\n",
        "    fig = plt.figure()\n",
        "    for ic in range(10):\n",
        "        ind_vec = np.zeros_like(classes)\n",
        "        ind_vec[:, ic] = 1\n",
        "        ind_class = classes[:, ic] == 1\n",
        "        color = plt.cm.Set1(ic)\n",
        "        plt.scatter(z_embed[ind_class, 0], z_embed[ind_class, 1], s=10, color=color)\n",
        "        plt.title(\"Latent Variable T-SNE per Class\")\n",
        "        fig.savefig('./vae_results/'+str(name)+'_embedding_'+str(ic)+'.png')\n",
        "    fig.savefig('./vae_results/'+str(name)+'_embedding.png')\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blqGYhCZ8Gb_"
      },
      "source": [
        "# Copyright (c) 2017-2019 Uber Technologies, Inc.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "from pyro.contrib.examples.util import MNIST\n",
        "\n",
        "# This file contains utilities for caching, transforming and splitting MNIST data\n",
        "# efficiently. By default, a PyTorch DataLoader will apply the transform every epoch\n",
        "# we avoid this by caching the data early on in MNISTCached class\n",
        "# https://github.com/pyro-ppl/pyro/blob/dev/examples/vae/utils/mnist_cached.py\n",
        "\n",
        "\n",
        "# transformations for MNIST data\n",
        "def fn_x_mnist(x, use_cuda):\n",
        "    # normalize pixel values of the image to be in [0,1] instead of [0,255]\n",
        "    xp = x * (1. / 255)\n",
        "\n",
        "    # transform x to a linear tensor from bx * a1 * a2 * ... --> bs * A\n",
        "    xp_1d_size = reduce(lambda a, b: a * b, xp.size()[1:])\n",
        "    xp = xp.view(-1, xp_1d_size)\n",
        "\n",
        "    # send the data to GPU(s)\n",
        "    if use_cuda:\n",
        "        xp = xp.cuda()\n",
        "\n",
        "    return xp\n",
        "\n",
        "\n",
        "def fn_y_mnist(y, use_cuda):\n",
        "    yp = torch.zeros(y.size(0), 10)\n",
        "\n",
        "    # send the data to GPU(s)\n",
        "    if use_cuda:\n",
        "        yp = yp.cuda()\n",
        "        y = y.cuda()\n",
        "\n",
        "    # transform the label y (integer between 0 and 9) to a one-hot\n",
        "    yp = yp.scatter_(1, y.view(-1, 1), 1.0)\n",
        "    return yp\n",
        "\n",
        "\n",
        "def get_ss_indices_per_class(y, sup_per_class):\n",
        "    # number of indices to consider\n",
        "    n_idxs = y.size()[0]\n",
        "\n",
        "    # calculate the indices per class\n",
        "    idxs_per_class = {j: [] for j in range(10)}\n",
        "\n",
        "    # for each index identify the class and add the index to the right class\n",
        "    for i in range(n_idxs):\n",
        "        curr_y = y[i]\n",
        "        for j in range(10):\n",
        "            if curr_y[j] == 1:\n",
        "                idxs_per_class[j].append(i)\n",
        "                break\n",
        "\n",
        "    idxs_sup = []\n",
        "    idxs_unsup = []\n",
        "    for j in range(10):\n",
        "        np.random.shuffle(idxs_per_class[j])\n",
        "        idxs_sup.extend(idxs_per_class[j][:sup_per_class])\n",
        "        idxs_unsup.extend(idxs_per_class[j][sup_per_class:len(idxs_per_class[j])])\n",
        "\n",
        "    return idxs_sup, idxs_unsup\n",
        "\n",
        "\n",
        "def split_sup_unsup_valid(X, y, sup_num, validation_num=10000):\n",
        "    \"\"\"\n",
        "    helper function for splitting the data into supervised, un-supervised and validation parts\n",
        "    :param X: images\n",
        "    :param y: labels (digits)\n",
        "    :param sup_num: what number of examples is supervised\n",
        "    :param validation_num: what number of last examples to use for validation\n",
        "    :return: splits of data by sup_num number of supervised examples\n",
        "    \"\"\"\n",
        "\n",
        "    # validation set is the last 10,000 examples\n",
        "    X_valid = X[-validation_num:]\n",
        "    y_valid = y[-validation_num:]\n",
        "\n",
        "    X = X[0:-validation_num]\n",
        "    y = y[0:-validation_num]\n",
        "\n",
        "    assert sup_num % 10 == 0, \"unable to have equal number of images per class\"\n",
        "\n",
        "    # number of supervised examples per class\n",
        "    sup_per_class = int(sup_num / 10)\n",
        "\n",
        "    idxs_sup, idxs_unsup = get_ss_indices_per_class(y, sup_per_class)\n",
        "    X_sup = X[idxs_sup]\n",
        "    y_sup = y[idxs_sup]\n",
        "    X_unsup = X[idxs_unsup]\n",
        "    y_unsup = y[idxs_unsup]\n",
        "\n",
        "    return X_sup, y_sup, X_unsup, y_unsup, X_valid, y_valid\n",
        "\n",
        "\n",
        "def print_distribution_labels(y):\n",
        "    \"\"\"\n",
        "    helper function for printing the distribution of class labels in a dataset\n",
        "    :param y: tensor of class labels given as one-hots\n",
        "    :return: a dictionary of counts for each label from y\n",
        "    \"\"\"\n",
        "    counts = {j: 0 for j in range(10)}\n",
        "    for i in range(y.size()[0]):\n",
        "        for j in range(10):\n",
        "            if y[i][j] == 1:\n",
        "                counts[j] += 1\n",
        "                break\n",
        "    print(counts)\n",
        "\n",
        "\n",
        "class MNISTCached(MNIST):\n",
        "    \"\"\"\n",
        "    a wrapper around MNIST to load and cache the transformed data\n",
        "    once at the beginning of the inference\n",
        "    \"\"\"\n",
        "\n",
        "    # static class variables for caching training data\n",
        "    train_data_size = 50000\n",
        "    train_data_sup, train_labels_sup = None, None\n",
        "    train_data_unsup, train_labels_unsup = None, None\n",
        "    validation_size = 10000\n",
        "    data_valid, labels_valid = None, None\n",
        "    test_size = 10000\n",
        "\n",
        "    def __init__(self, mode, sup_num, use_cuda=True, *args, **kwargs):\n",
        "        super().__init__(train=mode in [\"sup\", \"unsup\", \"valid\"], *args, **kwargs)\n",
        "\n",
        "        # transformations on MNIST data (normalization and one-hot conversion for labels)\n",
        "        def transform(x):\n",
        "            return fn_x_mnist(x, use_cuda)\n",
        "\n",
        "        def target_transform(y):\n",
        "            return fn_y_mnist(y, use_cuda)\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "        assert mode in [\"sup\", \"unsup\", \"test\", \"valid\"], \"invalid train/test option values\"\n",
        "\n",
        "        if mode in [\"sup\", \"unsup\", \"valid\"]:\n",
        "\n",
        "            # transform the training data if transformations are provided\n",
        "            if transform is not None:\n",
        "                self.data = (transform(self.data.float()))\n",
        "            if target_transform is not None:\n",
        "                self.targets = (target_transform(self.targets))\n",
        "\n",
        "            if MNISTCached.train_data_sup is None:\n",
        "                if sup_num is None:\n",
        "                    assert mode == \"unsup\"\n",
        "                    MNISTCached.train_data_unsup, MNISTCached.train_labels_unsup = \\\n",
        "                        self.data, self.targets\n",
        "                else:\n",
        "                    MNISTCached.train_data_sup, MNISTCached.train_labels_sup, \\\n",
        "                        MNISTCached.train_data_unsup, MNISTCached.train_labels_unsup, \\\n",
        "                        MNISTCached.data_valid, MNISTCached.labels_valid = \\\n",
        "                        split_sup_unsup_valid(self.data, self.targets, sup_num)\n",
        "\n",
        "            if mode == \"sup\":\n",
        "                self.data, self.targets = MNISTCached.train_data_sup, MNISTCached.train_labels_sup\n",
        "            elif mode == \"unsup\":\n",
        "                self.data = MNISTCached.train_data_unsup\n",
        "\n",
        "                # making sure that the unsupervised labels are not available to inference\n",
        "                self.targets = (torch.Tensor(\n",
        "                    MNISTCached.train_labels_unsup.shape[0]).view(-1, 1)) * np.nan\n",
        "            else:\n",
        "                self.data, self.targets = MNISTCached.data_valid, MNISTCached.labels_valid\n",
        "\n",
        "        else:\n",
        "            # transform the testing data if transformations are provided\n",
        "            if transform is not None:\n",
        "                self.data = (transform(self.data.float()))\n",
        "            if target_transform is not None:\n",
        "                self.targets = (target_transform(self.targets))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        :param index: Index or slice object\n",
        "        :returns tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        if self.mode in [\"sup\", \"unsup\", \"valid\"]:\n",
        "            img, target = self.data[index], self.targets[index]\n",
        "        elif self.mode == \"test\":\n",
        "            img, target = self.data[index], self.targets[index]\n",
        "        else:\n",
        "            assert False, \"invalid mode: {}\".format(self.mode)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "def setup_data_loaders(dataset, use_cuda, batch_size, sup_num=None, root=None, download=True, **kwargs):\n",
        "    \"\"\"\n",
        "        helper function for setting up pytorch data loaders for a semi-supervised dataset\n",
        "    :param dataset: the data to use\n",
        "    :param use_cuda: use GPU(s) for training\n",
        "    :param batch_size: size of a batch of data to output when iterating over the data loaders\n",
        "    :param sup_num: number of supervised data examples\n",
        "    :param download: download the dataset (if it doesn't exist already)\n",
        "    :param kwargs: other params for the pytorch data loader\n",
        "    :return: three data loaders: (supervised data for training, un-supervised data for training,\n",
        "                                  supervised data for testing)\n",
        "    \"\"\"\n",
        "    # instantiate the dataset as training/testing sets\n",
        "    if root is None:\n",
        "        root = get_data_directory('./')\n",
        "    if 'num_workers' not in kwargs:\n",
        "        kwargs = {'num_workers': 0, 'pin_memory': False}\n",
        "\n",
        "    cached_data = {}\n",
        "    loaders = {}\n",
        "    for mode in [\"unsup\", \"test\", \"sup\", \"valid\"]:\n",
        "        if sup_num is None and mode == \"sup\":\n",
        "            # in this special case, we do not want \"sup\" and \"valid\" data loaders\n",
        "            return loaders[\"unsup\"], loaders[\"test\"]\n",
        "        cached_data[mode] = dataset(root=root, mode=mode, download=download,\n",
        "                                    sup_num=sup_num, use_cuda=use_cuda)\n",
        "        loaders[mode] = DataLoader(cached_data[mode], batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "    return loaders\n",
        "\n",
        "\n",
        "def mkdir_p(path):\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError as exc:  # Python >2.5\n",
        "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
        "            pass\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "\n",
        "EXAMPLE_DIR = os.path.dirname(os.path.abspath(os.path.join('./', os.pardir)))\n",
        "DATA_DIR = os.path.join(EXAMPLE_DIR, 'data')\n",
        "RESULTS_DIR = os.path.join(EXAMPLE_DIR, 'results')\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhZYCalioDGG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7gBPcyx7i6x"
      },
      "source": [
        "# define the PyTorch module that parameterizes the\n",
        "# diagonal gaussian distribution q(z|x)\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, z_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        # multivariate normal has unexpected behavior with\n",
        "        # pyro.sample so we learn a \"good enough\" diagonal covariance matrix\n",
        "        # as a function of self.sigma\n",
        "        # softplus ensures the learned covariance is PSD\n",
        "        self.Tau = nn.Linear(1, z_dim) # z_scale = self.softplus(self.Tau(self.sigma**2))\n",
        "        self.softplus = nn.Softplus()\n",
        "        # self.M_inv_tilde = nn.Linear(z_dim, z_dim)\n",
        "\n",
        "        self.z_dim = z_dim\n",
        "        \n",
        "    def compute_M_inv(self, W, sigma):\n",
        "        # compute true precision matrix M_inv\n",
        "        # which is used to compute z_loc = M_inv W.T (x - mu)\n",
        "        # (bottom) (p. 573) http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf\n",
        "\n",
        "        # M = W.T W + sigma**2 I\n",
        "        M = torch.einsum('ij,jk->ik', W.T, W) + sigma**2 * torch.eye(self.z_dim)\n",
        "        # print(M.shape) # 50 x 50\n",
        "\n",
        "        # The inverse can be computed using the following identity\n",
        "        # (A + BD**(-1)C)**(-1) = A**(-1) ‚àí A**(-1)B(D + CA**(-1)B)**(-1)CA**(-1)\n",
        "        # setting\n",
        "        # B = W.T, D = I, C = W, A = sigma**2 I\n",
        "        # we have\n",
        "        # M_inv = (W.T W + sigma**2 I)**(-1) \n",
        "        # = (sigma**-2 I) ‚àí (sigma**-2 I)W.T(I + W(sigma**-2 I)W.T)**(-1) W(sigma**-2 I)\n",
        "\n",
        "        first_term = sigma**(-2) * torch.eye(self.z_dim)\n",
        "        # print(first_term.shape) # 50 x 50\n",
        "        sigma_neg2_WT = torch.einsum('ij,jk->ik', first_term, W.T)\n",
        "        # print(sigma_neg2_WT.shape) # 50 x 784\n",
        "        W_sigma_neg2_WT = torch.einsum('ij,jk->ik', W, sigma_neg2_WT)\n",
        "        # print(W_sigma_neg2_WT.shape) # 784 x 784\n",
        "        I_plus_W_sigma_neg2_WT_inv = torch.linalg.inv(torch.eye(784) + W_sigma_neg2_WT)\n",
        "        # print(I_plus_W_sigma_neg2_WT_inv.shape) # 784 x 784\n",
        "        second_term_aux = torch.einsum('ij,jk->ik', sigma_neg2_WT, I_plus_W_sigma_neg2_WT_inv)\n",
        "        # print(second_term_aux.shape) # 50 x 784\n",
        "        second_term = torch.einsum('ij,jk->ik', second_term_aux, sigma_neg2_WT.T)\n",
        "        # print(second_term.shape) # 50 x 50\n",
        "        M_inv = first_term - second_term\n",
        "        # print(self.M_inv.shape) # 50 x 50\n",
        "\n",
        "        return M, M_inv\n",
        "        \n",
        "    def forward(self, x, W, mu, sigma):\n",
        "        # define the forward computation on the image x\n",
        "        # first shape the mini-batch to have pixels in the rightmost dimension\n",
        "        x = x.reshape(-1, 784)\n",
        "\n",
        "        # compute z_loc \n",
        "        # (bottom) (p. 573) http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf\n",
        "        x_minus_mu = x - mu\n",
        "        # print(W.T.shape, x_minus_mu.shape) # 50x784, 256x784\n",
        "        WT_x_minus_mu = torch.einsum('ij,bj->bi', W.T, x_minus_mu)\n",
        "        # print(WT_x_minus_mu.shape) # 256x50\n",
        "        M, M_inv = self.compute_M_inv(W, sigma)\n",
        "        z_loc = torch.einsum('ij,bj->bi', M_inv, WT_x_minus_mu)\n",
        "        # z_loc = self.M_inv_tilde(WT_x_minus_mu)\n",
        "\n",
        "        # approximate covariance with learned diagonal covariance\n",
        "        z_scale = sigma**(-2) * M\n",
        "        # print(self.sigma.shape, sigma2_M.reshape(-1).shape)\n",
        "        # z_scale = self.softplus(self.Tau(self.sigma**(-2)))\n",
        "\n",
        "        return z_loc, z_scale"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obvz8gV8oGaa"
      },
      "source": [
        "# define the PyTorch module that parameterizes the\n",
        "# diagonal gaussian distribution q(z|x)\n",
        "class Encode_nn(nn.Module):\n",
        "    def __init__(self, z_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        # W : projection matrix\n",
        "        # mu : mean of x (is this needed? images are normalized)\n",
        "        # sigma : noise parameter\n",
        "        self.W = torch.rand(784, z_dim, requires_grad=True)\n",
        "        self.mu = torch.rand(784, requires_grad=True)\n",
        "        self.sigma = torch.rand(1, requires_grad=True)\n",
        "\n",
        "        # multivariate normal has unexpected behavior with\n",
        "        # pyro.sample so we learn a \"good enough\" diagonal covariance matrix\n",
        "        # as a function of self.sigma\n",
        "        # softplus ensures the learned covariance is PSD\n",
        "        self.Tau = nn.Linear(1, z_dim) # z_scale = self.softplus(self.Tau(self.sigma**2))\n",
        "        self.softplus = nn.Softplus()\n",
        "        # also approximate M_inv\n",
        "        self.M_inv_tilde = nn.Linear(z_dim, z_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # define the forward computation on the image x\n",
        "        # first shape the mini-batch to have pixels in the rightmost dimension\n",
        "        x = x.reshape(-1, 784)\n",
        "\n",
        "        # compute z_loc \n",
        "        # (bottom) (p. 573) http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf\n",
        "        x_minus_mu = x - self.mu\n",
        "        # print(self.W.T.shape, x_minus_mu.shape) # 50x784, 256x784\n",
        "        WT_x_minus_mu = torch.einsum('ij,bj->bi', self.W.T, x_minus_mu)\n",
        "        # print(WT_x_minus_mu.shape) # 256x50\n",
        "        z_loc = self.M_inv_tilde(WT_x_minus_mu)\n",
        "\n",
        "        # approximate covariance with learned diagonal covariance\n",
        "        z_scale = self.softplus(self.Tau(self.sigma**2))\n",
        "\n",
        "        return z_loc, z_scale"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLOdu-KwoGK4"
      },
      "source": [
        "# define the PyTorch module that parameterizes the\n",
        "# observation likelihood p(x|z)\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, z_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        # setup the two linear transformations used\n",
        "        # self.W = torch.rand(784, z_dim, requires_grad=True)\n",
        "        # self.mu = torch.rand(784, requires_grad=True)\n",
        "\n",
        "        # self.fc21 = nn.Linear(hidden_dim, 784)\n",
        "        # setup the non-linearities\n",
        "        # self.softplus = nn.Softplus()\n",
        "\n",
        "    def forward(self, z, W, mu):\n",
        "        # define the forward computation on the latent z\n",
        "        # first compute the hidden units\n",
        "        # hidden = self.softplus(self.fc1(z))\n",
        "        # return the parameter for the output Bernoulli\n",
        "        # each is of size batch_size x 784\n",
        "        # loc_img = torch.sigmoid(self.fc21(hidden))\n",
        "        x_minus_mu = torch.einsum('ij,bj->bi', W, z)\n",
        "        loc_img = torch.sigmoid(x_minus_mu + mu)\n",
        "        return loc_img"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb7HKu98YsQA"
      },
      "source": [
        "# define a PyTorch module for the VAE\n",
        "class VAE(nn.Module):\n",
        "    # by default our latent space is 50-dimensional\n",
        "    # and we use 400 hidden units\n",
        "    def __init__(self, z_dim=50, hidden_dim=400, use_cuda=False):\n",
        "        super().__init__()\n",
        "        # create the encoder and decoder networks\n",
        "        self.encoder = Encoder(z_dim, hidden_dim)\n",
        "        self.decoder = Decoder(z_dim, hidden_dim)\n",
        "\n",
        "        # W : projection matrix\n",
        "        # mu : mean of x (is this needed? images are normalized)\n",
        "        # sigma : noise parameter\n",
        "        self.W = torch.randn(784, z_dim, requires_grad=True)\n",
        "        self.mu = torch.zeros(784)\n",
        "        self.sigma = torch.randn(1, requires_grad=True)\n",
        "\n",
        "        if use_cuda:\n",
        "            # calling cuda() here will put all the parameters of\n",
        "            # the encoder and decoder networks into gpu memory\n",
        "            self.cuda()\n",
        "        self.use_cuda = use_cuda\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "    # define the model p(x|z)p(z)\n",
        "    def model(self, x):\n",
        "        # register PyTorch module `decoder` with Pyro\n",
        "        pyro.module(\"decoder\", self.decoder)\n",
        "        pyro.param(\"W\", self.W)\n",
        "        # pyro.param(\"mu\", self.mu)\n",
        "        pyro.param(\"sigma\", self.sigma)\n",
        "        # For a model with ùëÅ observations, running the model and guide and constructing the ELBO \n",
        "        # involves evaluating log pdf‚Äôs whose complexity scales badly with ùëÅ. \n",
        "        # This is a problem if we want to scale to large datasets. \n",
        "        # Luckily, the ELBO objective naturally supports subsampling provided that our \n",
        "        # model/guide have some conditional independence structure that we can take advantage of. \n",
        "        # For example, in the case that the observations are conditionally independent given the latents, \n",
        "        # the log likelihood term in the ELBO can be approximated with mini-batch\n",
        "        # when there are only local random variables the scale factor introduced by subsampling scales\n",
        "        # all the terms in the ELBO by the same amount. This explains why for the VAE it‚Äôs permissible\n",
        "        # for the user to take complete control over subsampling and pass mini-batches directly to the model and guide\n",
        "        # ; plate is still used, but subsample_size and subsample are not.\n",
        "        with pyro.plate(\"data\", x.shape[0]): \n",
        "            # setup hyperparameters for prior p(z)\n",
        "            z_loc = torch.zeros(x.shape[0], self.z_dim, dtype=x.dtype, device=x.device)\n",
        "            z_scale = torch.eye(self.z_dim, dtype=x.dtype, device=x.device)\n",
        "            # sample from prior (value will be sampled by guide when computing the ELBO)\n",
        "            # normal = dist.Normal(z_loc, z_scale).to_event(1)\n",
        "            # print(normal.event_shape, normal.batch_shape) # 50, 256\n",
        "            # z = pyro.sample(\"latent\", normal)\n",
        "            # print(z.shape)\n",
        "            mvn = dist.MultivariateNormal(z_loc, z_scale)\n",
        "            # print(mvn.event_shape, mvn.batch_shape) # 50, 256\n",
        "            z = pyro.sample(\"latent\", mvn)\n",
        "            # print(z_mvn.shape)\n",
        "\n",
        "            # decode the latent code z\n",
        "            loc_img = self.decoder.forward(z, self.W, self.mu)\n",
        "            # score against actual images (with relaxed Bernoulli values)\n",
        "            # the use of .to_event(1) when sampling from the latent z ensures \n",
        "            # that instead of treating our sample as being generated from a univariate normal \n",
        "            # with batch_size = z_dim, we treat them as being generated from a \n",
        "            # multivariate normal distribution with diagonal covariance.\n",
        "            # As such, the log probabilities along each dimension is summed \n",
        "            # out when we evaluate .log_prob for a ‚Äúlatent‚Äù sample.\n",
        "            pyro.sample(\"obs\",\n",
        "                        dist.Bernoulli(loc_img, validate_args=False)\n",
        "                            .to_event(1),\n",
        "                        obs=x.reshape(-1, 784))\n",
        "            # return the loc so we can visualize it later\n",
        "            return loc_img\n",
        "\n",
        "    # define the guide (i.e. variational distribution) q(z|x)\n",
        "    def guide(self, x):\n",
        "        # register PyTorch module `encoder` with Pyro\n",
        "        pyro.module(\"encoder\", self.encoder)\n",
        "        with pyro.plate(\"data\", x.shape[0]):\n",
        "            # use the encoder to get the parameters used to define q(z|x)\n",
        "            z_loc, z_scale = self.encoder.forward(x, self.W, self.mu, self.sigma)\n",
        "            # print(z_loc.shape, z_scale.shape)\n",
        "            # sample the latent code z\n",
        "            # normal = dist.Normal(z_loc, z_scale).to_event(1)\n",
        "            # pyro.sample(\"latent\", normal)\n",
        "            # print(normal.event_shape, normal.batch_shape) # 50, 256\n",
        "            # z = pyro.sample(\"latent2\", normal)\n",
        "            # print(z.shape) # 256 x 50\n",
        "            mvn = dist.MultivariateNormal(z_loc, z_scale)\n",
        "            # print(mvn.event_shape, mvn.batch_shape) # 50, 256\n",
        "            pyro.sample(\"latent\", mvn)\n",
        "            # print(z.shape)\n",
        "\n",
        "    # define a helper function for reconstructing images\n",
        "    def reconstruct_img(self, x):\n",
        "        # encode image x\n",
        "        z_loc, z_scale = self.encoder(x, self.W, self.mu, self.sigma)\n",
        "        # sample in latent space\n",
        "        z = dist.Normal(z_loc, z_scale).sample()\n",
        "        # decode the image (note we don't sample in image space)\n",
        "        loc_img = self.decoder(z, self.W, self.mu)\n",
        "        return loc_img"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM0LMEMdoJQf"
      },
      "source": [
        "# define a PyTorch module for the VAE\n",
        "class VAE_normal(nn.Module):\n",
        "    # by default our latent space is 50-dimensional\n",
        "    # and we use 400 hidden units\n",
        "    def __init__(self, z_dim=50, hidden_dim=400, use_cuda=False):\n",
        "        super().__init__()\n",
        "        # create the encoder and decoder networks\n",
        "        self.encoder = Encoder(z_dim, hidden_dim)\n",
        "        self.decoder = Decoder(z_dim, hidden_dim)\n",
        "\n",
        "        if use_cuda:\n",
        "            # calling cuda() here will put all the parameters of\n",
        "            # the encoder and decoder networks into gpu memory\n",
        "            self.cuda()\n",
        "        self.use_cuda = use_cuda\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "    # define the model p(x|z)p(z)\n",
        "    def model(self, x):\n",
        "        # register PyTorch module `decoder` with Pyro\n",
        "        pyro.module(\"decoder\", self.decoder)\n",
        "        # For a model with ùëÅ observations, running the model and guide and constructing the ELBO \n",
        "        # involves evaluating log pdf‚Äôs whose complexity scales badly with ùëÅ. \n",
        "        # This is a problem if we want to scale to large datasets. \n",
        "        # Luckily, the ELBO objective naturally supports subsampling provided that our \n",
        "        # model/guide have some conditional independence structure that we can take advantage of. \n",
        "        # For example, in the case that the observations are conditionally independent given the latents, \n",
        "        # the log likelihood term in the ELBO can be approximated with mini-batch\n",
        "        # when there are only local random variables the scale factor introduced by subsampling scales\n",
        "        # all the terms in the ELBO by the same amount. This explains why for the VAE it‚Äôs permissible\n",
        "        # for the user to take complete control over subsampling and pass mini-batches directly to the model and guide\n",
        "        # ; plate is still used, but subsample_size and subsample are not.\n",
        "        with pyro.plate(\"data\", x.shape[0]): \n",
        "            # setup hyperparameters for prior p(z)\n",
        "            z_loc = torch.zeros(x.shape[0], self.z_dim, dtype=x.dtype, device=x.device)\n",
        "            z_scale = torch.ones(x.shape[0], self.z_dim, dtype=x.dtype, device=x.device)\n",
        "            # sample from prior (value will be sampled by guide when computing the ELBO)\n",
        "            z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
        "            # decode the latent code z\n",
        "            loc_img = self.decoder.forward(z)\n",
        "            # score against actual images (with relaxed Bernoulli values)\n",
        "            # the use of .to_event(1) when sampling from the latent z ensures \n",
        "            # that instead of treating our sample as being generated from a univariate normal \n",
        "            # with batch_size = z_dim, we treat them as being generated from a \n",
        "            # multivariate normal distribution with diagonal covariance.\n",
        "            # As such, the log probabilities along each dimension is summed \n",
        "            # out when we evaluate .log_prob for a ‚Äúlatent‚Äù sample.\n",
        "            pyro.sample(\"obs\",\n",
        "                        dist.Bernoulli(loc_img, validate_args=False)\n",
        "                            .to_event(1),\n",
        "                        obs=x.reshape(-1, 784))\n",
        "            # return the loc so we can visualize it later\n",
        "            return loc_img\n",
        "\n",
        "    # define the guide (i.e. variational distribution) q(z|x)\n",
        "    def guide(self, x):\n",
        "        # register PyTorch module `encoder` with Pyro\n",
        "        pyro.module(\"encoder\", self.encoder)\n",
        "        with pyro.plate(\"data\", x.shape[0]):\n",
        "            # use the encoder to get the parameters used to define q(z|x)\n",
        "            z_loc, z_scale = self.encoder.forward(x)\n",
        "            # sample the latent code z\n",
        "            pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
        "\n",
        "    # define a helper function for reconstructing images\n",
        "    def reconstruct_img(self, x):\n",
        "        # encode image x\n",
        "        z_loc, z_scale = self.encoder(x)\n",
        "        # sample in latent space\n",
        "        z = dist.Normal(z_loc, z_scale).sample()\n",
        "        # decode the image (note we don't sample in image space)\n",
        "        loc_img = self.decoder(z)\n",
        "        return loc_img"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naNMKRVK7zqc",
        "outputId": "f3855ee6-ad3d-4281-cf25-21d1b196e8b5"
      },
      "source": [
        "# Copyright (c) 2017-2019 Uber Technologies, Inc.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "pyro.distributions.enable_validation(False)\n",
        "pyro.set_rng_seed(0)\n",
        "\n",
        "def main(args):\n",
        "    # clear param store\n",
        "    pyro.clear_param_store()\n",
        "\n",
        "    # setup MNIST data loaders\n",
        "    # train_loader, test_loader\n",
        "    train_loader, test_loader = setup_data_loaders(MNISTCached, use_cuda=args.cuda, batch_size=256)\n",
        "\n",
        "    # setup the VAE\n",
        "    vae = VAE(use_cuda=args.cuda)\n",
        "\n",
        "    # setup the optimizer\n",
        "    adam_args = {\"lr\": args.learning_rate}\n",
        "    optimizer = Adam(adam_args)\n",
        "\n",
        "    # setup the inference algorithm\n",
        "    elbo = JitTrace_ELBO() if args.jit else Trace_ELBO()\n",
        "    svi = SVI(vae.model, vae.guide, optimizer, loss=elbo)\n",
        "\n",
        "    # training loop\n",
        "    train_test_loop(args, train_loader, test_loader, svi, vae)\n",
        "\n",
        "    return vae\n",
        "\n",
        "\n",
        "def train_test_loop(args, train_loader, test_loader, svi, vae):\n",
        "    # setup visdom for visualization\n",
        "    if args.visdom_flag:\n",
        "        vis = visdom.Visdom()\n",
        "    else:\n",
        "        vis = None\n",
        "\n",
        "    train_elbo = []\n",
        "    test_elbo = []\n",
        "\n",
        "    for epoch in range(args.num_epochs):\n",
        "        total_epoch_loss_train = train(train_loader, svi)\n",
        "        train_elbo.append(total_epoch_loss_train)\n",
        "        print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
        "\n",
        "        if epoch % args.test_frequency == 0:\n",
        "            total_epoch_loss_test = evaluate(args, test_loader, svi, vae, vis)\n",
        "            test_elbo.append(total_epoch_loss_test)\n",
        "            print(\"[epoch %03d]  average test loss: %.4f\" % (epoch, total_epoch_loss_test))\n",
        "\n",
        "        if epoch == args.tsne_iter:\n",
        "            mnist_test_tsne(vae=vae, test_loader=test_loader)\n",
        "            plot_llk(np.array(train_elbo), np.array(test_elbo))\n",
        "\n",
        "\n",
        "def train(train_loader, svi):\n",
        "    # initialize loss accumulator\n",
        "    epoch_loss = 0.\n",
        "    # do a training epoch over each mini-batch x returned\n",
        "    # by the data loader\n",
        "    for x, _ in train_loader:\n",
        "        # if on GPU put mini-batch into CUDA memory\n",
        "        if args.cuda:\n",
        "            x = x.cuda()\n",
        "        # do ELBO gradient and accumulate loss\n",
        "        epoch_loss += svi.step(x)\n",
        "\n",
        "    # report training diagnostics\n",
        "    normalizer_train = len(train_loader.dataset)\n",
        "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
        "\n",
        "    return total_epoch_loss_train\n",
        "\n",
        "\n",
        "def evaluate(args, test_loader, svi, vae, vis):\n",
        "    # initialize loss accumulator\n",
        "    test_loss = 0.\n",
        "    # compute the loss over the entire test set\n",
        "    for i, (x, _) in enumerate(test_loader):\n",
        "        # if on GPU put mini-batch into CUDA memory\n",
        "        if args.cuda:\n",
        "            x = x.cuda()\n",
        "        # compute ELBO estimate and accumulate loss\n",
        "        test_loss += svi.evaluate_loss(x)\n",
        "\n",
        "        # pick three random test images from the first mini-batch and\n",
        "        # visualize how well we're reconstructing them\n",
        "        if i == 0:\n",
        "            if args.visdom_flag:\n",
        "              plot_vae_samples(vae, vis)\n",
        "              reco_indices = np.random.randint(0, x.shape[0], 3)\n",
        "              for index in reco_indices:\n",
        "                  test_img = x[index, :]\n",
        "                  reco_img = vae.reconstruct_img(test_img)\n",
        "                  vis.image(test_img.reshape(28, 28).detach().cpu().numpy(),\n",
        "                            opts={'caption': 'test image'})\n",
        "                  vis.image(reco_img.reshape(28, 28).detach().cpu().numpy(),\n",
        "                            opts={'caption': 'reconstructed image'})\n",
        "\n",
        "    # report test diagnostics\n",
        "    normalizer_test = len(test_loader.dataset)\n",
        "    total_epoch_loss_test = test_loss / normalizer_test\n",
        "\n",
        "    return total_epoch_loss_test\n",
        "    \n",
        "\n",
        "\n",
        "sys.argv = ['foo']\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"parse args\")\n",
        "parser.add_argument('-n', '--num-epochs', default=101, type=int, help='number of training epochs')\n",
        "parser.add_argument('-tf', '--test-frequency', default=10, type=int, help='how often we evaluate the test set')\n",
        "parser.add_argument('-lr', '--learning-rate', default=1.0e-3, type=float, help='learning rate')\n",
        "parser.add_argument('--cuda', action='store_true', default=False, help='whether to use cuda')\n",
        "parser.add_argument('--jit', action='store_true', default=False, help='whether to use PyTorch jit')\n",
        "parser.add_argument('-visdom', '--visdom_flag', action=\"store_true\", help='Whether plotting in visdom is desired')\n",
        "parser.add_argument('-i-tsne', '--tsne_iter', default=100, type=int, help='epoch when tsne visualization runs')\n",
        "args = parser.parse_args()\n",
        "\n",
        "model = main(args)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[epoch 000]  average training loss: 38471.2846\n",
            "[epoch 000]  average test loss: 27884.1285\n",
            "[epoch 001]  average training loss: 22786.5611\n",
            "[epoch 002]  average training loss: 16355.0928\n",
            "[epoch 003]  average training loss: 12781.8993\n",
            "[epoch 004]  average training loss: 10461.0182\n",
            "[epoch 005]  average training loss: 8726.2500\n",
            "[epoch 006]  average training loss: 7265.8326\n",
            "[epoch 007]  average training loss: 5765.6455\n",
            "[epoch 008]  average training loss: 4016.2855\n",
            "[epoch 009]  average training loss: 2318.3348\n",
            "[epoch 010]  average training loss: 1312.0207\n",
            "[epoch 010]  average test loss: 1028.9420\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cn2GsrpQHfrM"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image('vae_results/VAE_embedding_8.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVOJRNhcPRFt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwzxYiVdaKnY"
      },
      "source": [
        "!ls vae_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_vhGAxtCPGe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}